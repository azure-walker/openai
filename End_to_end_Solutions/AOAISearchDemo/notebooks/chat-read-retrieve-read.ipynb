{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from util.env_variables import Env\n",
    "\n",
    "e = Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client_secret_credential():\n",
    "    # [START create_client_secret_credential]\n",
    "    from azure.identity import ClientSecretCredential\n",
    "\n",
    "    credential = ClientSecretCredential(\n",
    "        tenant_id=e.tenant_id,\n",
    "        client_id=e.client_id,\n",
    "        client_secret=e.client_secret\n",
    "    )\n",
    "\n",
    "    return credential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import QueryType\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Replace these with your own values, either in environment variables or directly here\n",
    "AZURE_STORAGE_ACCOUNT               = os.environ.get(\"AZURE_STORAGE_ACCOUNT\") or \"mystorageaccount\"\n",
    "AZURE_STORAGE_CONTAINER             = os.environ.get(\"AZURE_STORAGE_CONTAINER\") or \"content\"\n",
    "AZURE_SEARCH_SERVICE                = os.environ.get(\"AZURE_SEARCH_SERVICE\") or \"gptkb\"\n",
    "AZURE_SEARCH_INDEX                  = os.environ.get(\"AZURE_SEARCH_INDEX\") or \"gptkbindex\"\n",
    "AZURE_OPENAI_SERVICE                = os.environ.get(\"AZURE_OPENAI_SERVICE\") or \"myopenai\"\n",
    "AZURE_OPENAI_GPT_DEPLOYMENT         = os.environ.get(\"AZURE_OPENAI_GPT_DEPLOYMENT\") or \"davinci\"\n",
    "#AZURE_OPENAI_CHATGPT_DEPLOYMENT     = os.environ.get(\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\") or \"chat\"\n",
    "AZURE_OPENAI_CHATGPT_DEPLOYMENT     = os.environ.get(\"AZURE_OPENAI_CLASSIFIER_DEPLOYMENT\") or \"chat\"\n",
    "\n",
    "KB_FIELDS_CONTENT = os.environ.get(\"KB_FIELDS_CONTENT\") or \"content\"\n",
    "KB_FIELDS_CATEGORY = os.environ.get(\"KB_FIELDS_CATEGORY\") or \"category\"\n",
    "KB_FIELDS_SOURCEPAGE = os.environ.get(\"KB_FIELDS_SOURCEPAGE\") or \"sourcepage\"\n",
    "\n",
    "# Use the current user identity to authenticate with Azure OpenAI, Cognitive Search and Blob Storage (no secrets needed, \n",
    "# just use 'az login' locally, and managed identity when deployed on Azure). If you need to use keys, use separate AzureKeyCredential instances with the \n",
    "# keys for each service\n",
    "#azure_credential = DefaultAzureCredential()\n",
    "#azure_credential = credential   \n",
    "azure_credential = DefaultAzureCredential()\n",
    "#credential=create_client_secret_credential()\n",
    "\n",
    "# Used by the OpenAI SDK\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = f\"https://{AZURE_OPENAI_SERVICE}.openai.azure.com\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "\n",
    "# Comment these two lines out if using keys, set your API key in the OPENAI_API_KEY environment variable instead\n",
    "openai.api_type = \"azure_ad\"\n",
    "openai.api_key = azure_credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "#openai.api_key = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "\n",
    "#   search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))\n",
    "# Set up clients for Cognitive Search and Storage\n",
    "search_client = SearchClient(\n",
    "    endpoint=f\"https://{AZURE_SEARCH_SERVICE}.search.windows.net\",\n",
    "    index_name=AZURE_SEARCH_INDEX,\n",
    "    credential=AzureKeyCredential(os.getenv(\"search_query_key\")))\n",
    "    #credential=os.getenv(\"search_query_key\"))\n",
    "    #credential=AzureKeyCredential(openai.api_key))\n",
    "    #credential=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6Ii1LSTNROW5OUjdiUm9meG1lWm9YcWJIWkdldyIsImtpZCI6Ii1LSTNROW5OUjdiUm9meG1lWm9YcWJIWkdldyJ9.eyJhdWQiOiJodHRwczovL2NvZ25pdGl2ZXNlcnZpY2VzLmF6dXJlLmNvbSIsImlzcyI6Imh0dHBzOi8vc3RzLndpbmRvd3MubmV0LzBiNjlhYjQwLTFiYzctNDY2Ni05ZjIwLTY5MWJhMTA1YTkwNy8iLCJpYXQiOjE2OTAzMDg0OTgsIm5iZiI6MTY5MDMwODQ5OCwiZXhwIjoxNjkwMzEyMzk4LCJhaW8iOiJFMlpnWUdoNkxaM0QrVlcwYVZualA4dXd5NFl2QUE9PSIsImFwcGlkIjoiMjkyMjQ4MjItZTBiZS00NDU1LWJlMmQtY2E1MmM2MTc1MDhjIiwiYXBwaWRhY3IiOiIxIiwiaWRwIjoiaHR0cHM6Ly9zdHMud2luZG93cy5uZXQvMGI2OWFiNDAtMWJjNy00NjY2LTlmMjAtNjkxYmExMDVhOTA3LyIsIm9pZCI6ImQ1NGY5OTEwLTFlYWMtNDkyNi04MGQxLTQxODZiYmRmMzRiNiIsInJoIjoiMC5BWHdBUUt0cEM4Y2Jaa2FmSUdrYm9RV3BCNUFpTVgzSUtEeEhvTzJPVTNTYmJXM01BQUEuIiwic3ViIjoiZDU0Zjk5MTAtMWVhYy00OTI2LTgwZDEtNDE4NmJiZGYzNGI2IiwidGlkIjoiMGI2OWFiNDAtMWJjNy00NjY2LTlmMjAtNjkxYmExMDVhOTA3IiwidXRpIjoiR0JyQ2RONlJ4RVNRNVZnaGh5NEFBQSIsInZlciI6IjEuMCJ9.JcX5vlscy-8wJcq-oq_qNnEY4-ykw9L3tn9O_VeB0uF7JEvASo2d7kSK5wYiFYyKpfv7TbS4N8nksyDeyrCkcwXuam6QlIFNZL9n8yO5YpdDYn6Sfl9bSnLYhVBrsaTkHKpU0IqCiU_yorVU7kawRl5WxuHZ9bRKIPDkya5tZ6MTJAmLjnFu56wrwwg_4VPUKux_2JtWg5a6ySESboyWSRc8iEkfe6IFDPnfOQcY1c-OMsY9n5Nwr9u6J5Rmi0sEEoj7AK1DLsJ0zY846ToxN3Fh-IchAsAgi10pcgLp99WmEpzFGFkcLlv8RKmMOkGoG0Z46JSd4HBFAkV9qKiTEQ'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT uses a particular set of tokens to indicate turns in conversations\n",
    "prompt_prefix = \"\"\"<|im_start|>system\n",
    "Assistant helps the company employees with their healthcare plan questions and employee handbook questions. \n",
    "Answer ONLY with the facts listed in the list of sources below. If there isn't enough information below, say you don't know. Do not generate answers that don't use the sources below. If asking a clarifying question to the user would help, ask the question. \n",
    "Each source has a name followed by colon and the actual information, always include the source name for each fact you use in the response. Use square brakets to reference the source, e.g. [info1.txt]. Don't combine sources, list each source separately, e.g. [info1.txt][info2.pdf].\n",
    "\n",
    "Sources:\n",
    "{sources}\n",
    "\n",
    "<|im_end|>\"\"\"\n",
    "\n",
    "turn_prefix = \"\"\"\n",
    "<|im_start|>user\n",
    "\"\"\"\n",
    "\n",
    "turn_suffix = \"\"\"\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "prompt_history = turn_prefix\n",
    "\n",
    "history = []\n",
    "\n",
    "summary_prompt_template = \"\"\"Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base. Generate a search query based on the conversation and the new question. Source names are not good search terms to include in the search query.\n",
    "\n",
    "Summary:\n",
    "{summary}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Search query:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell multiple times updating user_input to accumulate chat history\n",
    "from util.toolbox import popup_choice\n",
    "\n",
    "prompt_history=\"\"\n",
    "def ask_your_data_a_question(user_input = \"What healthcare plans are available?\"):\n",
    "\n",
    "    #user_input = \"What is my out of pocket expenses to visit a doctor?\"\n",
    "    #user_input = \"Does my plan cover annual physicals?\"\n",
    "    #user_input = \"How many days of vacation do i get\"\n",
    "\n",
    "    # Exclude category, to simulate scenarios where there's a set of docs you can't see\n",
    "    exclude_category = None\n",
    "\n",
    "    if len(history) > 0:\n",
    "        completion = openai.Completion.create(\n",
    "            engine=AZURE_OPENAI_GPT_DEPLOYMENT,\n",
    "            prompt=summary_prompt_template.format(summary=\"\\n\".join(history), question=user_input),\n",
    "            temperature=0.7,\n",
    "            max_tokens=32,\n",
    "            stop=[\"\\n\"])\n",
    "        search = completion.choices[0].text\n",
    "    else:\n",
    "        search = user_input\n",
    "\n",
    "\n",
    "    # Alternatively simply use search_client.search(q, top=3) if not using semantic search\n",
    "    print(\"Searching:\", search)\n",
    "    print(\"-------------------\")\n",
    "    filter = \"category ne '{}'\".format(exclude_category.replace(\"'\", \"''\")) if exclude_category else None\n",
    "    r = search_client.search(search, \n",
    "                            filter=filter,\n",
    "                            query_type=QueryType.SEMANTIC, \n",
    "                            query_language=\"en-us\", \n",
    "                            query_speller=\"lexicon\", \n",
    "                            semantic_configuration_name=\"default\", \n",
    "                            top=3)\n",
    "\n",
    "\n",
    "    results = [doc[KB_FIELDS_SOURCEPAGE] + \": \" + doc[KB_FIELDS_CONTENT].replace(\"\\n\", \"\").replace(\"\\r\", \"\") for doc in r]\n",
    "    content = \"\\n\".join(results)\n",
    "\n",
    "\n",
    "    prompt = prompt_prefix.format(sources=content) + prompt_history + user_input + turn_suffix\n",
    "\n",
    "    completion = openai.Completion.create(\n",
    "        engine=AZURE_OPENAI_CHATGPT_DEPLOYMENT, \n",
    "        prompt=prompt, \n",
    "        temperature=0.7, \n",
    "        max_tokens=1024,\n",
    "        stop=[\"<|im_end|>\", \"<|im_start|>\"])\n",
    "\n",
    "    prompt_history += user_input + turn_suffix + completion.choices[0].text + \"\\n<|im_end|>\" + turn_prefix\n",
    "    history.append(\"user: \" + user_input)\n",
    "    history.append(\"assistant: \" + completion.choices[0].text)\n",
    "\n",
    "    print(\"\\n-------------------\\n\".join(history))\n",
    "    print(\"\\n-------------------\\nPrompt:\\n\" + prompt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching: How many doctors visits am I allowed\n",
      "-------------------\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'prompt_history' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ask_your_data_a_question(\u001b[39m\"\u001b[39;49m\u001b[39mHow many doctors visits am I allowed\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m, in \u001b[0;36mask_your_data_a_question\u001b[0;34m(user_input)\u001b[0m\n\u001b[1;32m     39\u001b[0m results \u001b[39m=\u001b[39m [doc[KB_FIELDS_SOURCEPAGE] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m doc[KB_FIELDS_CONTENT]\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m r]\n\u001b[1;32m     40\u001b[0m content \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(results)\n\u001b[0;32m---> 43\u001b[0m prompt \u001b[39m=\u001b[39m prompt_prefix\u001b[39m.\u001b[39mformat(sources\u001b[39m=\u001b[39mcontent) \u001b[39m+\u001b[39m prompt_history \u001b[39m+\u001b[39m user_input \u001b[39m+\u001b[39m turn_suffix\n\u001b[1;32m     45\u001b[0m completion \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m     46\u001b[0m     engine\u001b[39m=\u001b[39mAZURE_OPENAI_CHATGPT_DEPLOYMENT, \n\u001b[1;32m     47\u001b[0m     prompt\u001b[39m=\u001b[39mprompt, \n\u001b[1;32m     48\u001b[0m     temperature\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, \n\u001b[1;32m     49\u001b[0m     max_tokens\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m,\n\u001b[1;32m     50\u001b[0m     stop\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m<|im_end|>\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m<|im_start|>\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     52\u001b[0m prompt_history \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m user_input \u001b[39m+\u001b[39m turn_suffix \u001b[39m+\u001b[39m completion\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m<|im_end|>\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m turn_prefix\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'prompt_history' referenced before assignment"
     ]
    }
   ],
   "source": [
    "ask_your_data_a_question(\"How many doctors visits am I allowed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c40b9fc8dfc687e53ddb074d322e19207ef9cf3db51c580aef67976913dea803"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
